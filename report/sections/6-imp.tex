
\subsection{Preprocessor}

The preprocessor in Kite is almost identical to the \code{C} language preprocessor. We use a library called \code{cpphs}\footnote{\url{http://projects.haskell.org/cpphs}} that mimics the \code{C} version, and provides an embeddable Haskell library. This is the reason for the \code{\#include "file.kite"} syntax. Preprocessing is the first step of the compilation, and generates a single source code file which is a recursive concatenation of all included source files referenced from the main file.

\subsection{Lexer}
\label{sec:impl-lexer}
The lexer converts the raw input source to a list of tokens, a process known as tokenization. There are multiple types of tokens that represent different kinds of lexical elements. A \emph{lexeme} is the textual value that the lexer sees and maps to a token. Some tokens can have multiple possible lexemes, for instance in the case of identifiers and numeric constants, so in those cases the lexeme is saved together with the token for further use (by the parser).

Kite's lexer is implemented using the lexical generator Alex. Alex is a tool for generating lexical analyzers in Haskell, given a description of the tokens to be recognized in the form of regular expressions\cite[p. 4]{dornan01}. The description of the analyzer consists of \emph{macro definitions} and \emph{rules}. A macro definition is either a regular expression bound to an identifier describing a particular sequence of characters, denoted by a \code{\$} prefix, or it is a combination of the former prefixed by \code{@}. These macros are combined to make up the rules, which define the actual definitions for tokens. Macros and rules are separated by the symbol \code{:-} and the preceding \code{kite} is just for documentation\cite[p. 7]{dornan01}. Further the description file defines the Haskell data structures used to represent the tokens, enclosed in \code{\{ \}}.

Figure~\ref{fig:lexer} shows an excerpt from the description file. The macro definitions describe common patterns such as a sequence of digits (\code{\$digit}), lowercase characters (\code{\$downcase}), alpha numeric sequences (\code{\$alphaNum}) etc. Also defined are reserved keywords (\code{\@keywords}) and the void type (\code{@void}). Another interesting macro is the \code{@comment} macro, which matches two dashes followed by anything (implicitly anything but a newline).

The macros are then used in the rules to define tokens. A rule consist of a combination of macros followed by a Haskell code block that produce the token data structure. The code block must be a function accepting a position data structure and the matched lexeme. The position node is used to keep track of tokens where found in the source file to be able to give useful error messages. The lexeme is sometimes coerced to another value matching the one required by the respective token structure. The \code{@void} rule defines the \code{Void} type which is the only token possible from that rule, and therefore it does not use the provided lexeme.

Note that the (\code{@comment}) rule defines no code block but just a \code{;} which means that the pattern is matched, consumed and ignored, which is exactly what we want for comments.

At the bottom we see the data constructors used to create the tokens.

\begin{figure}[p]
\begin{lstlisting}
$downcase		= a-z
$upcase			= A-Z
$digit			= 0-9
$alpha			= [$downcase $upcase]
$alphaNum		= [$alpha $digit]

@keywords		= return | if | then | else | match
@identifier		= $downcase [$alphaNum \_ \' \! \?]*
@void           = Void
@comment		= "--" .*

kite :-
  @comment		    ;
  @keywords		    { \p s -> TKeyword p s }
  $digit+\.$digit+	{ \p s -> TFloat p (read s) }
  $digit+		    { \p s -> TInteger p (read s) }
  @identifier		{ \p s -> TIdentifier p s }
  @void		        { \p s -> TVoid p }

{
data Token = TIdentifier AlexPosn String
           | TInteger    AlexPosn Int
           | TFloat      AlexPosn Float
           | TKeyword    AlexPosn String
           | TVoid       AlexPosn
           deriving (Eq,Show)
}
\end{lstlisting}
\label{fig:lexer}
\caption{Excerpt from the lexer description file}
\end{figure}

Kite uses 12 different tokens briefly described in table \ref{tbl:lexical_tokens}.
\begin{table}[H]
  \centering
  \begin{tabular}{lll}
    \hline
    Token      & Description                            & Sample lexemes    \\ \hline

    Symbol     & Single character symbols & \code{;, !} \\ \hline
    Identifier & Identifiers for referencable variables & \code{map, x', \_foobar} \\ \hline
    Type       & Capitalized identifier, denoting a type construct & \code{Bool, Int, Void} \\ \hline
    Integer    & Integer sequence & \code{0, 1, 1337}\\ \hline
    Float      & Floating point values & \code{0.0, 3.14, 2f} \\ \hline
    Bool       & Boolean values & \code{True, False} \\ \hline
    Void       & The void type & \code{Void} \\ \hline
    String     & Sequence of characters enclosed in \code{``''} & \code{``Hello, world!''} \\ \hline
    Char       & Single character enclosed in \code{\'} & \code{'a', '!', ' '} \\ \hline
    Keyword    & Reserved keywords used in the Kite syntax & \code{if, return, match} \\ \hline
    Operator   & List of symbol characters & \code{=, /, <=, !!} \\ \hline
    EOF        & The end of file marker & n/a as it's non-visual
  \end{tabular}

  \label{tbl:lexical_tokens}
  \caption{Tokens recognized by the lexical analyzer}
\end{table}

\subsection{Parser}
The parser is implemented with the LALR-parser (\textbf{L}ook \textbf{A}head, \textbf{L}eft-to-Right, \textbf{R}ightmost derivation) generator Happy, which is a parser generator system for Haskell, similar to the tool yacc\footnote{Yacc: Yet Another Compiler-Compiler: \url{http://dinosaur.compilertools.net/yacc}} for C. It takes a file containing a specification of a grammar and produces a Haskell module containing a parser for the grammar\cite{marlow01}. The grammar file (\code{Parser.y}), is similar in format to the lexical analysis description file described in section \ref{sec:impl-lexer}. It uses Backus-Naur Form (BNF) notation to define a context-free grammar specifying the legal syntax of the Kite language. A BNF grammar consists of a set of derivation rules, also known as production rules, which describe all legal combinations of the tokens read by the lexer. A production rule has the following format

\begin{lstlisting}
Name :: { Type }
      : Expression_1 { Haskell code }
      | Expression_2 { Haskell code }
      ...
      | Expression_n { Haskell code }
\end{lstlisting}

The \code{Type} specifies the type of Haskell data constructor that the rule will produce and is optional but useful for debugging purposes and readability. A rule can have multiple different legal expressions, separated by \code{|}, which are defined as a sequence of symbols. A symbol can be either a \emph{terminal} or another production rule. A terminal is an expression that cannot be further expanded, thus terminating the recursion of that branch. Consider for instance the production rule for the bind syntax (the bind node is named \code{PBind} in the Haskell code)

\begin{lstlisting}
Bind :: { Expr }
        : id '=' Expr                  { PBind $1 $3 }
        | '{' operator '}' '=' Expr    { PBind $2 $5 }
\end{lstlisting}

The \code{Bind} rule produces a node of type \code{Expr}. In the first choice we see the \code{id} terminal that is equivalent to the \code{Identifier} token from the lexer. Other terminals are \code{'='}, \code{'\{'} and \code{'\}'}. The right-hand side of the rules are the code snippets that defines what Happy will generate when a rule is matched, where \code{\$n} means that the nth symbol of the BNF notated grammar, will be inserted. In the first rule this means that a \code{PBind} will be created with the \code{id} as the first argument and the \code{Expr} rules as the second.

Production rules can be recursively defined, so that an expression can contain the rule itself. Consider for instance the grammar for defining lists

\begin{lstlisting}
Exprs :: { [Expr] }
        : {- nothing -}    { [] }
        | Expr             { [$1] }
        | Expr ',' Exprs   { $1 : $3 }

List   :: { Expr }
        : '[' Exprs ']'           { PList $2 }
\end{lstlisting}

The \code{Exprs} rule defines a comma-separated list of \code{Expr} rules. The \code{\{- nothing -\}} defined the empty match, equivalent to the $\epsilon$ common mathematical notation.

The first rule in the grammar defines the entry point of the parser and thus the resulting type that the generated parser will produce. In the case of Kite the entry rule is \code{Program} and produced the type \code{[Decl]}.

\paragraph{Syntactic sugar}
Some matches are first transformed into temporary syntactic sugaring structures before being inserted into the parse tree. This is described further in section \ref{sec:imp-sugar}.

Happy generates more than 2,000 lines of quite unreadable Haskell code, which is used in the compilation of the compiler. This is not useful for debugging, but fortunately Happy can produce an information file by setting the \code{--info} flag when run, which will generate a \code{.info}-file. This file yields very useful information about shift-reduce and reduce-reduce conflicts, which has been quite helpful throughout the development.

\paragraph{Shift-reduce parsing}
Happy generates a shift-reduce parser. This means that when it reads a symbol it can either reduce the symbol thus ``ending'' a rule and producing a result, or it can shift the symbol and look for more symbols to match before reducing. This is the point of a Look-Ahead parser, since it can look at the next symbol to be consumed before deciding what to do. Happy generates LALR(1) which means that it looks 1 symbol ahead.

A common hurdle when implementing shift-reduce context-free grammars (and indeed parsers in general) is conflicts, or ambiguities, in the grammar. There are two types of conflict, namely \emph{shift/reduce} and \emph{reduce/reduce} conflicts. A reduce/reduce conflict occurs if there are two or more rules that apply to the same sequence of symbols\cite[sec 5.6]{bison13}.


When the parser sees a symbol and can \emph{reduce} to two different rules, it is called a reduce-reduce conflict. This is usually a critical problem because the parser has no reasonable way of choosing which rule to reduce to. Happy simply reduces to the rule defined first. Below is the most simple example of a reduce/reduce conflict (in simplified BNF notation)

\begin{lstlisting}
Foo : 'a'
Bar : 'a'
\end{lstlisting}

When seeing a \code{'a'} the parser cannot decide whether to reduce to \code{Foo} or \code{Bar}.

A shift-reduce conflict happens when the parser has the option to either reduce to a rule or shift the symbol and continue. This is much less critical because the obvious choice is always to shift. Consider the following example of a

\begin{lstlisting}
Foo : 'a' 'b'
Bar : 'a'
\end{lstlisting}

Here the parser, when seeing an \code{'a'} as the current symbol and \code{'b'} as the look-ahead symbol, it can either reduce the \code{Bar} rule or shift the \code{'a'} and continue parsing the \code{'b'}.

\subsection{Syntactic sugar}
\label{sec:imp-sugar}
Because of the simplistic nature of the Kite AST it would be terse to write programs that directly translates to AST nodes. The \code{Desugar} module allows Kite programs to be written in a succinct manner and further enables a powerful feature known as \emph{list comprehensions}. It makes the language ``sweeter'' for human use; things can be expressed more clearly, more concisely, or in an alternative style that some may prefer.\cite{wiki-sugar14}. The process of converting sugared syntax to standard form is called \emph{desugaring} of code.

We have decided to integrate desugaring directly in the parser because we avoid the need for an intermediate AST representation to be desugared after parsing. The \code{Desugar} module defines functions to desugar specific cases of sugar, that are used in the Haskell code snippets. For instance in the rule for function application, \code{Apply}:

\begin{lstlisting}
Apply   :: { Expr }
        : Expr '(' Exprs ')'    { mkCalls $1 $3 }
        | Expr '`' Expr Expr    { PApply (PApply $3 $1) $4 }
        | ...
\end{lstlisting}

Here the function \code{mkCalls} is defined in the \code{Desugar} module and converts multiple arguments to a function to a series of applications. Next we give a more detailed description of all the syntactic sugar supported in Kite. Most of the desugaring is straight forward and the details of their use is described in section \ref{sec:kite-design-sugar}, but we will explain the most interesting parts next

\paragraph{Strings}
As the representation of strings outputted from the lexer is a Haskell \code{String} type (which is a type synonym for \code{[Char]}), we simply map the \code{PChar} constructor over each letter and construct a \code{PList} containing the resulting list. This way a string end of having the type \code{PList PChar} in the AST.

\paragraph{Multiple arguments in function application}
The \code{mkCalls} function converts a function call with multiple arguments to a sequence of applications. The Haskell code is the following

\begin{haskell}

mkCalls f [] = PApply f PVoid
mkCalls f (a:as) = foldl PApply (PApply f a) as
\end{haskell}

Where \code{(a:as)} is the destructured\footnote{Destructuring means breaking up a data structure into its parts by using pattern matching. Kite features a simple implementation of this in it's \code{match} expressions} list of arguments, and we reduce the list by folding with a \code{PApply} data constructor.

\paragraph{List comprehensions}
As list comprehensions are composed of an output expression, draws and guards, the desugaring is implemented as follows (for an example of a list comprehension, and its desugared version, see section \ref{sec:ex-listcomp}):

\begin{enumerate}

\item The identifiers from the draws are extracted as these will be used as arguments to the various functions.

\item A flatMap\footnote{\code{flatMap} takes as arguments a lambda-expression and a list, maps the function to the list, and \code{flatten}s the result} is generated for each of the draws, with the function taking as argument the identifier of the current draw and the list-expression as defined on the right-hand side of the \code{<-}, e.g. \code{[1, 2, 3]} in \code{x <- [1, 2, 3]}. The body of the lambda expression is either a nested flatMap or the final if-expression of generated from the guards:

\item The guards are inserted as bodies into functions which take all the extracted identifiers as arguments. These lambda-expressions are then conjoined in the condition of the \code{if}-expression.

\item The \code{then}-branch of the \code{if}-expression is a function with the output expression as its body and the extracted identifiers as its arguments. The \code{else}-branch is simply an empty list. This implies that only the elements that pass the guards are outputted, as flatMap gets rid of empty elements.

\end{enumerate}

\subsection{Analyzer}
TODO: analyzer in general

\subsubsection{Type inference}

As mentioned earlier, Kite uses static type checking to verify that types align at compile-time. We have used the Hindley-Milner type inference algorithm first described by Damas and Milner \cite{milner82}. They described an algorithm, named ``Algorithm W'', that given an expression will infer the most general type of that expression. We introduce the $\vdash$ to mean a derivation of types from an expression. For instance $x = 1 \vdash x : Int$ is read as ``given the expression $x = 1$ we can derive that $x$ has type $Int$''. To give a sense of what the algorithm does we first present a few examples. Recall that $:$ is pronounced ``has type''

Let the following be predefined
\begin{align*}
  +      & : a \to a \to a   \quad\text{(infix)}\\
  head   & : [a] \to a   \\
  fst    & : (a, b) \to a
\end{align*}

The algorithm can now infer the types of the following expressions
\begin{align*}
  \fn{xs}{head(xs) + 1} & \qcol [Int] \to Int    & \qvd xs : [Int]             \\
  \fn{p}{fst(p) + 1}    & \qcol (Int, a) \to Int & \qvd p : (Int, a)           \\
  f(n + 1)              & \qcol a                & \qvd f: (Int \to a), n: Int \\
\end{align*}

Using the last example above the intuition behind the algorithm is as follows

\begin{enumerate}
\item $e = \fn{f, n}{\ldots} \vdash e: (a \to b \to c)$ \\
  $e$ is being assigned a lambda expression with two parameters but we do not know anything about their types
\item $e = \fn{f, n}{f(\ldots)} \vdash f: d \to c$ \\
  $f$ is being applied to a single (yet unknown) value, thus we can infer that it is a function
\item $e = \fn{f, n}{f(n + 1)} \vdash n : Int$ \\
  We see that $f$ is applied to $n+1$ and since $1: Int$ and $+:a \to a \to a$ then $a = Int$ and thus $n:Int$ (this is called type unification, explained below).
\item $e = \fn{f, n}{f(n + 1)} \vdash e : (Int \to c) \to Int \to c$ \\
  There are no more expressions to infer so we end up with the final, most general type for $e$
\end{enumerate}

\paragraph{Type unification}

We define the $\tau$ function to ``type of'', such that for instance from the above $\tau(e) = (Int \to c) \to Int \to c$.

When the type of $n$ was inferred, we \emph{unified} the known type of $+:a \to a \to a$ with the expression $n + 1$. Note that $n + 1$ is equivalent to the prefix form $+(n, 1)$. From a base case (shown below) we know that $\tau(1) = Int = a$. This gives us

. This is called type unification and is at the core of the algorithm.

Formally the unification algorithm takes two types and either fails or returns a \emph{substitution} that maps the most general type to the most specific of the two. The algorithm is here formulated in Haskell code and is equivalent to that of our actual implementation (though simplified and not compilable)

%\begin{haskell}
%-- Note:
%-- <+> is an infix function that composes (union) two substituions
%-- nullSubst is an empty substituion
%
%unify :: Type -> Type -> String -> TC Substitution
%
%-- primitive base cases
%-- empty substitutions
%unify IntegerType IntegerType = return nullSubst
%unify FloatType FloatType     = return nullSubst
%unify CharType CharType       = return nullSubst
%unify BoolType BoolType       = return nullSubst
%unify VoidType VoidType       = return nullSubst
%
%-- type var with any type
%-- binds the type var name to the other type
%unify ta (PTypeVar name) = varBind name ta
%unify (PTypeVar name) tb = varBind name ta
%
%-- list
%unify (PListType ta) (PListType tb) = unify ta tb
%
%-- pair
%unify (PPairType ta tb) (PPairType ta' tb') = do
%  sa = unify ta ta'
%  sb = unify tb tb'
%  return (sa <+> sb)
%
%-- lambda
%unify (PLambdaType paramA returnA) (PLambdaType paramB returnB) =
%  sParam <- unify paramB paramA
%  sReturn <- unify (apply sParam ra) (apply sParam rb)
%  return (sParam <+> s2)
%
%-- if nothing matched it's an error
%unify ta tb err = throwTE (printf err (show ta) (show tb))
%
%-- perform occurs check
%-- creates a substituion
%varBind :: Name -> Type -> TC Substitution
%varBind ide t | t == PTypeVar ide = return nullSubst
%              | ide `Set.member` ftv t = throwTE $ "Occurs in type: " ++ ide ++ " vs. " ++ show t
%              | otherwise = return $ Map.singleton ide t
%
%\end{haskell}


\paragraph{Limitations of Hindley-Milner}
While the Hindley-Milner algorithm certainly is powerful and elegant, it has (in it's original form) some limitations that contrain the type system. Most notably we cannot support subtyping, meaning that we cannot define a type as being an extension of another. Subtyping in \code{Java} is declared using the \code{extends} keyword, thus making a \code{class} a subtype of another class (called the supertype).

Subtyping is not possible (or difficult at the least) due to the fact that the unification algorithm cannot tell the most general type of an expression if subtypes are allowed.

Consider the function $foo : Person \to Int$. Now, if we have the type $Manager <: Person$ (meaning $Manager$ is a subtype of $Person$) and we want to infer %TODO

%TODO: lexer ~ regex ~ lexeme
%TODO: lexical scoping
%TODO: optimization inlining

\subsection{Optimizer}

The optimizer implemented in Kite is currently only a simple dead-code elimination algorithm. The algorithm is given a starting declaration (usually the \code{main} declaration) and recursively traverses the AST beginning with the expressions defined in the starting node. When an identifier node is detected (except in a bind node) it's name is saved and recursion continues. The part of the AST that has been traversed, is the derivation tree that will be executed when running the program, thus all referenced identifiers will have been detected. The full list of declarations is now filtered by only persisting the ones that were detected during traversal, since we can be certain that they will never be accessed.

In it's current state the algorithm only eliminates unused top-level declarations, thus leaving unused locally scoped variables in the code. The algorithm can however be extended to eliminate local variables by transforming the AST during traversal. Using a stack of used identifiers we could, when entering a new local scope (a lambda or match case), push a new frame to the stack, add accessed variables found in the current scope to it, and when leaving the scope filter out the variables that were not accessed.

TODO - mention `target'-specific optimization - standard binary math functions get emitted nicer - TODO Groupleader AKA Simon

\subsection{Code generation}

The implementation of code generation is split into parts for LLVM and JavaScript.

\textbf{LLVM as backend:} The generation can be done using LLVM
bindings for Haskell. 


\textbf{JavaScript as backend:}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:

