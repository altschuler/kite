TODO: Implementation of all 9 compiler-parts


\subsection{Preprocessor}

The preprocessor in Kite is almost identical to the \code{C} language preprocessor. We use a library called \code{cpphs}\footnote{\url{http://projects.haskell.org/cpphs}} that mimics the \code{C} version, and provides an embeddable Haskell library. This is the reason for the \code{\#include "file.kite"} syntax. Preprocessing is the first step of the compilation, and generates a single source code file which is a recursive concatenation of all included source files referenced from the main file.

\subsection{Lexer}
The lexer is implemented with the lexical generator Alex \cite{dornan01}. The lexer splits in

\subsection{Parser}
The parser is implemented with the LALR-parser generator Happy \cite{marlow01}. Happy automatically generates Haskell code from the file `src/Kite/Parser.y', which we have written in partially Haskell and partially Backus-Naur Form\footnote{This is a well known notation technique for specifying context-free grammars, which the more popular parser generator ANTLR also makes use of.}.

When a(/a combination of) tokens are matched in the parser, it is transformed into Haskell data-structures, from which we build the parse tree. Some matches are first transformed into temporary syntactic sugaring structures before being inserted into the parse tree. This is described further in section \ref{sec:imp-sugar}

The current parser generates more than 2,000 lines of quite unreadable Haskell code, which is used in the compilation of the compiler. Fortunately Happy allows setting a --info flag when it is used, which also generates a .info-file about the parser. For instance it yields very useful information about shift-reduce and reduce-reduce conflicts, which has been a great throughout the development.


\subsection{Syntactic sugar}
\label{sec:imp-sugar}
We have implemented a desugaring module which receives the matched tokens and converts them into the parse tree data-structures. We have implemented the following syntactic sugar:

\begin{itemize}
  \item Infix operators
  \item Strings
  \item List comprehensions
    
    As list comprehensions are composed of an output expression, draws and guards, the desugaring is implemented as follows (for an example of a list comprehension, and its desugared version,  please see section \ref{sec:ex-listcomp}):
    \begin{description}
      \item[First] the identifiers from the draws are extracted as these will be used as arguments to the various functions.
      \item[Secondly,] a flatMap\footnote{flatMaps takes as arguments a lambda-expression and a list, maps the function to the list, and flattens the result} is generated for each of the draws, with the function taking as argument the identifier of the current draw and the list-expression as defined right-hand-side of the ` \texttt{<-}', e.g. `\texttt{x <- [1, 2, 3]}' or `\texttt{x <- range(1,3)}'. The body of the lambda expression is either a nested flatMap or an if-expression (TODO: the final step?).
        \item[Thirdly,] the guards are inserted as bodies to functions which take all the extracted identifiers as arguments. These lambda-expressions are then combined as conjunctions in the condition of the \textbf{if}-expression.
      \item[Finally,] the \textbf{then}-branch of the \textbf{if}-expression is the output expression, and the \textbf{else}-branch is simply an empty list. This implies that only the elements that pass the guards are outputted, as flatMap gets rid of empty elements.
    \end{description}

    
\end{itemize}

TODO: infix

TODO: strings

TODO: multiple parameters

\subsection{Analyser}

\subsubsection{Type inference}

As mentioned earlier, Kite uses static type checking to verify that types align at compile-time. We have used the Hindley-Milner type inference algorithm first described by Damas and Milner \cite{milner82}. They described an algorithm, named ``Algorithm W'', that given an expression will infer the most general type of that expression. We introduce the $\vdash$ to mean a derivation of types from an expression. For instance $x = 1 \vdash x : Int$ is read as ``given the expression $x = 1$ we can derive that $x$ has type $Int$''. To give a sense of what the algorithm does we first present a few examples. Recall that $:$ is pronounced ``has type''

Let the following be predefined
\begin{align*}
  +      & : a \to a \to a   \quad\text{(infix)}\\
  head   & : [a] \to a   \\
  fst    & : (a, b) \to a
\end{align*}

The algorithm can now infer the types of the following expressions
\begin{align*}
  \fn{xs}{head(xs) + 1} & \qcol [Int] \to Int    & \qvd xs : [Int]             \\
  \fn{p}{fst(p) + 1}    & \qcol (Int, a) \to Int & \qvd p : (Int, a)           \\
  f(n + 1)              & \qcol a                & \qvd f: (Int \to a), n: Int \\
\end{align*}

Using the last example above the intuition behind the algorithm is as follows

\begin{enumerate}
\item $e = \fn{f, n}{\ldots} \vdash e: (a \to b \to c)$ \\
  $e$ is being assigned a lambda expression with two parameters but we do not know anything about their types
\item $e = \fn{f, n}{f(\ldots)} \vdash f: d \to c$ \\
  $f$ is being applied to a single (yet unknown) value, thus we can infer that it is a function
\item $e = \fn{f, n}{f(n + 1)} \vdash n : Int$ \\
  We see that $f$ is applied to $n+1$ and since $1: Int$ and $+:a \to a \to a$ then $a = Int$ and thus $n:Int$ (this is called type unification, explained below).
\item $e = \fn{f, n}{f(n + 1)} \vdash e : (Int \to c) \to Int \to c$ \\
  There are no more expressions to infer so we end up with the final, most general type for $e$
\end{enumerate}

\paragraph{Type unification}

We define the $\tau$ function to ``type of'', such that for instance from the above $\tau(e) = (Int \to c) \to Int \to c$.

When the type of $n$ was inferred, we \emph{unified} the known type of $+:a \to a \to a$ with the expression $n + 1$. Note that $n + 1$ is equivalent to the prefix form $+(n, 1)$. From a base case (shown below) we know that $\tau(1) = Int = a$. This gives us

. This is called type unification and is at the core of the algorithm.

Formally the unification algorithm takes two types and either fails or returns a \emph{substitution} that maps the most general type to the most specific of the two. The algorithm is here formulated in Haskell code and is equivalent to that of our actual implementation (though simplified and not compilable)

\begin{haskell}[]
-- Note:
-- <+> is an infix function that composes (union) two substituions
-- nullSubst is an empty substituion

unify :: Type -> Type -> String -> TC Substitution

-- primitive base cases
-- empty substitutions
unify IntegerType IntegerType = return nullSubst
unify FloatType FloatType     = return nullSubst
unify CharType CharType       = return nullSubst
unify BoolType BoolType       = return nullSubst
unify VoidType VoidType       = return nullSubst

-- type var with any type
-- binds the type var name to the other type
unify ta (PTypeVar name) = varBind name ta
unify (PTypeVar name) tb = varBind name ta

-- list
unify (PListType ta) (PListType tb) = unify ta tb

-- pair
unify (PPairType ta tb) (PPairType ta' tb') = do
  sa = unify ta ta'
  sb = unify tb tb'
  return (sa <+> sb)

-- lambda
unify (PLambdaType paramA returnA) (PLambdaType paramB returnB) =
  sParam <- unify paramB paramA
  sReturn <- unify (apply sParam ra) (apply sParam rb)
  return (sParam <+> s2)

-- if nothing matched it's an error
unify ta tb err = throwTE (printf err (show ta) (show tb))

-- perform occurs check
-- creates a substituion
varBind :: Name -> Type -> TC Substitution
varBind ide t | t == PTypeVar ide = return nullSubst
              | ide `Set.member` ftv t = throwTE $ "Occurs in type: " ++ ide ++ " vs. " ++ show t
              | otherwise = return $ Map.singleton ide t

\end{haskell}


\paragraph{Limitations of Hindley-Milner}
While the Hindley-Milner algorithm certainly is powerful and elegant, it has (in it's original form) some limitations that contrain the type system. Most notably we cannot support subtyping, meaning that we cannot define a type as being an extension of another. Subtyping in \code{Java} is declared using the \code{extends} keyword, thus making a \code{class} a subtype of another class (called the supertype).

Subtyping is not possible (or difficult at the least) due to the fact that the unification algorithm cannot tell the most general type of an expression if subtypes are allowed.

Consider the function $foo : Person \to Int$. Now, if we have the type $Manager <: Person$ (meaning $Manager$ is a subtype of $Person$) and we want to infer %TODO

%TODO: lexer ~ regex ~ lexeme
%TODO: lexical scoping
%TODO: optimization inlining


\subsection{Optimizer}

The optimizer implemented in Kite is currently only a simple dead-code elimination algorithm. The algorithm is given a starting declaration (usually the \code{main} declaration) and recursively traverses the AST beginning with the expressions defined in the starting node. When an identifier node is detected (except in a bind node) it's name is saved and recursion continues. The part of the AST that has been traversed, is the derivation tree that will be executed when running the program, thus all referenced identifiers will have been detected. The full list of declarations is now filtered by only persisting the ones that were detected during traversal, since we can be certain that they will never be accessed.

In it's current state the algorithm only eliminates unused top-level declarations, thus leaving unused locally scoped variables in the code. The algorithm can however be extended to eliminate local variables by transforming the AST during traversal. Using a stack of used identifiers we could, when entering a new local scope (a lambda or match case), push a new frame to the stack, add accessed variables found in the current scope to it, and when leaving the scope filter out the variables that were not accessed.

\subsection{Code generation}

The

TODO: (i language-deisgn) Write reserved names (Void, if, then, else, etc...)



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
