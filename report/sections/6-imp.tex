TODO: Implementation of all 9 compiler-parts


\subsection{Preprocessor}

The preprocessor in Kite is almost identical to the \code{C} language preprocessor. We use a library called \code{cpphs}\footnote{\url{http://projects.haskell.org/cpphs}} that mimics the \code{C} version, and provides an embeddable Haskell library. This is the reason for the \code{\#include "file.kite"} syntax. Preprocessing is the first step of the compilation, and generates a single source code file which is a recursive concatenation of all included source files referenced from the main file.

\subsection{Lexer}
The lexer converts the raw input source to a list of tokens, a process known as tokenization. There are multiple types of tokens that represent different kinds of lexical elements. A \emph{lexeme} is the textual value that the lexer sees and maps to a token. Some tokens can have multiple possible lexemes, for instance in the case of identifiers and numeric constants, so in those cases the lexeme is saved together with the token for further use (by the parser).

Kite's lexer is implemented using the lexical generator Alex. Alex is a tool for generating lexical analyzers in Haskell, given a description of the tokens to be recognized in the form of regular expressions\cite[p. 4]{dornan01}. The description of the analyzer consists of \emph{macro definitions} and \emph{rules}. A macro definition is either a regular expression bound to an identifier describing a particular sequence of characters, denoted by a \code{\$} prefix, or it is a combination of the former prefixed by \code{@}. These macros are combined to make up the rules, which define the actual definitions for tokens. Macros and rules are separated by the symbol \code{:-} and the preceding \code{kite} is just for documentation\cite[p. 7]{dornan01}. Further the description file defines the Haskell data structures used to represent the tokens, enclosed in \code{\{ \}}.

Figure~\ref{fig:lexer} shows an excerpt from the description file. The macro definitions describe common patterns such as a sequence of digits (\code{\$digit}), lowercase characters (\code{\$downcase}), alpha numeric sequences (\code{\$alphaNum}) etc. Also defined are reserved keywords (\code{\@keywords}) and the void type (\code{@void}). Another interesting macro is the \code{@comment} macro, which matches two dashes followed by anything (implicitly anything but a newline).

The macros are then used in the rules to define tokens. A rule consist of a combination of macros followed by a Haskell code block that produce the token data structure. The code block must be a function accepting a position data structure and the matched lexeme. The position node is used to keep track of tokens where found in the source file to be able to give useful error messages. The lexeme is sometimes coerced to another value matching the one required by the respective token structure. The \code{@void} rule defines the \code{Void} type which is the only token possible from that rule, and therefore it does not use the provided lexeme.

Note that the (\code{@comment}) rule defines no code block but just a \code{;} which means that the pattern is matched, consumed and ignored, which is exactly what we want for comments.

At the bottom we see the data constructors used to create the tokens.

\begin{figure}[p]
\begin{lstlisting}
$downcase		= a-z
$upcase			= A-Z
$digit			= 0-9
$alpha			= [$downcase $upcase]
$alphaNum		= [$alpha $digit]

@keywords		= return | if | then | else | match
@identifier		= $downcase [$alphaNum \_ \' \! \?]*
@void           = Void
@comment		= "--" .*

kite :-
  @comment		    ;
  @keywords		    { \p s -> TKeyword p s }
  $digit+\.$digit+	{ \p s -> TFloat p (read s) }
  $digit+		    { \p s -> TInteger p (read s) }
  @identifier		{ \p s -> TIdentifier p s }
  @void		        { \p s -> TVoid p }

{
data Token = TIdentifier AlexPosn String
           | TInteger    AlexPosn Int
           | TFloat      AlexPosn Float
           | TKeyword    AlexPosn String
           | TVoid       AlexPosn
           deriving (Eq,Show)
}
\end{lstlisting}
\label{fig:lexer}
\caption{Excerpt from the lexer description file}
\end{figure}

Kite uses 12 different tokens briefly described in table \ref{tbl:lexical_tokens}.
%TODO finish table
\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \hline
    Token      & Description                            & Sample lexemes    \\ \hline
    Symbol     & Single character symbols               & \code{;, !}              \\ \hline
    Identifier & Identifiers for referencable variables & \code{map, x', \_foobar} \\ \hline
  \end{tabular}

  \label{fig:lexer}
  \caption{Excerpt from the lexer description file}
\end{table}
TSymbol, TIdentifier, TType, TInteger, TFloat, TBool, TVoi, TString, TChar, TKeyword, TOperator, TEOF

\subsection{Parser}
The parser is implemented with the LALR-parser generator Happy \cite{marlow01}. Happy automatically generates Haskell code from the file `src/Kite/Parser.y', which we have written in partially Haskell and partially Backus-Naur Form\footnote{This is a well known notation technique for specifying context-free grammars, which the more popular parser generator ANTLR also makes use of.}.

When a(/a combination of) tokens are matched in the parser, it is transformed into Haskell data-structures, from which we build the parse tree. Some matches are first transformed into temporary syntactic sugaring structures before being inserted into the parse tree. This is described further in section \ref{sec:imp-sugar}

The current parser generates more than 2,000 lines of quite unreadable Haskell code, which is used in the compilation of the compiler. Fortunately Happy allows setting a --info flag when it is used, which also generates a .info-file about the parser. For instance it yields very useful information about shift-reduce and reduce-reduce conflicts, which has been a great throughout the development.


\subsection{Syntactic sugar}
\label{sec:imp-sugar}
We have implemented a desugaring module which receives the matched tokens and converts them into the parse tree data-structures. We have implemented the following syntactic sugar:

\begin{itemize}
  \item \textbf{Infix function application}
    
    This is implemented in the parser, as it simply takes the two arguments of the application and parses it as a normal function call with two arguments.

  \item \textbf{Multiple parameters and arguments}

    \texttt{foo(a,b,c)} (TODO: args) TODO: se LambdaSignatur i parser.y

    \texttt{bar = |x,y,z| -> { ... }} (TODO: params) TODO: se i parser under mkCalls



  \item \textbf{Strings}

    As the representation of `strings' outputted from the lexer is a Haskell-list, where each element is a letter, we just map each element to a `PChar' and wrap it within a `PList'. Thus it has become a list of characters in the parse tree.

  \item \textbf{List comprehensions}
    
    As list comprehensions are composed of an output expression, draws and guards, the desugaring is implemented as follows (for an example of a list comprehension, and its desugared version,  please see section \ref{sec:ex-listcomp}):
    \begin{description}
      \item[First] the identifiers from the draws are extracted as these will be used as arguments to the various functions.
      \item[Secondly,] a flatMap\footnote{flatMaps takes as arguments a lambda-expression and a list, maps the function to the list, and flattens the result} is generated for each of the draws, with the function taking as argument the identifier of the current draw and the list-expression as defined right-hand-side of the ` \texttt{<-}', e.g. `\texttt{[1, 2, 3]}' in `\texttt{x <- [1, 2, 3]}'. The body of the lambda expression is either a nested flatMap or the final if-expression.
        \item[Thirdly,] the guards are inserted as bodies into functions which take all the extracted identifiers as arguments. These lambda-expressions are then conjoined in the condition of the \textbf{if}-expression.
      \item[Finally,] the \textbf{then}-branch of the \textbf{if}-expression is a function with the output expression as its body and the extracted identifiers as its arguments. The \textbf{else}-branch is simply an empty list. This implies that only the elements that pass the guards are outputted, as flatMap gets rid of empty elements.
    \end{description}

    
\end{itemize}

TODO: infix

TODO: strings

TODO: multiple parameters

\subsection{Analyser}

\subsubsection{Type inference}

As mentioned earlier, Kite uses static type checking to verify that types align at compile-time. We have used the Hindley-Milner type inference algorithm first described by Damas and Milner \cite{milner82}. They described an algorithm, named ``Algorithm W'', that given an expression will infer the most general type of that expression. We introduce the $\vdash$ to mean a derivation of types from an expression. For instance $x = 1 \vdash x : Int$ is read as ``given the expression $x = 1$ we can derive that $x$ has type $Int$''. To give a sense of what the algorithm does we first present a few examples. Recall that $:$ is pronounced ``has type''

Let the following be predefined
\begin{align*}
  +      & : a \to a \to a   \quad\text{(infix)}\\
  head   & : [a] \to a   \\
  fst    & : (a, b) \to a
\end{align*}

The algorithm can now infer the types of the following expressions
\begin{align*}
  \fn{xs}{head(xs) + 1} & \qcol [Int] \to Int    & \qvd xs : [Int]             \\
  \fn{p}{fst(p) + 1}    & \qcol (Int, a) \to Int & \qvd p : (Int, a)           \\
  f(n + 1)              & \qcol a                & \qvd f: (Int \to a), n: Int \\
\end{align*}

Using the last example above the intuition behind the algorithm is as follows

\begin{enumerate}
\item $e = \fn{f, n}{\ldots} \vdash e: (a \to b \to c)$ \\
  $e$ is being assigned a lambda expression with two parameters but we do not know anything about their types
\item $e = \fn{f, n}{f(\ldots)} \vdash f: d \to c$ \\
  $f$ is being applied to a single (yet unknown) value, thus we can infer that it is a function
\item $e = \fn{f, n}{f(n + 1)} \vdash n : Int$ \\
  We see that $f$ is applied to $n+1$ and since $1: Int$ and $+:a \to a \to a$ then $a = Int$ and thus $n:Int$ (this is called type unification, explained below).
\item $e = \fn{f, n}{f(n + 1)} \vdash e : (Int \to c) \to Int \to c$ \\
  There are no more expressions to infer so we end up with the final, most general type for $e$
\end{enumerate}

\paragraph{Type unification}

We define the $\tau$ function to ``type of'', such that for instance from the above $\tau(e) = (Int \to c) \to Int \to c$.

When the type of $n$ was inferred, we \emph{unified} the known type of $+:a \to a \to a$ with the expression $n + 1$. Note that $n + 1$ is equivalent to the prefix form $+(n, 1)$. From a base case (shown below) we know that $\tau(1) = Int = a$. This gives us

. This is called type unification and is at the core of the algorithm.

Formally the unification algorithm takes two types and either fails or returns a \emph{substitution} that maps the most general type to the most specific of the two. The algorithm is here formulated in Haskell code and is equivalent to that of our actual implementation (though simplified and not compilable)

\begin{haskell}[]
-- Note:
-- <+> is an infix function that composes (union) two substituions
-- nullSubst is an empty substituion

unify :: Type -> Type -> String -> TC Substitution

-- primitive base cases
-- empty substitutions
unify IntegerType IntegerType = return nullSubst
unify FloatType FloatType     = return nullSubst
unify CharType CharType       = return nullSubst
unify BoolType BoolType       = return nullSubst
unify VoidType VoidType       = return nullSubst

-- type var with any type
-- binds the type var name to the other type
unify ta (PTypeVar name) = varBind name ta
unify (PTypeVar name) tb = varBind name ta

-- list
unify (PListType ta) (PListType tb) = unify ta tb

-- pair
unify (PPairType ta tb) (PPairType ta' tb') = do
  sa = unify ta ta'
  sb = unify tb tb'
  return (sa <+> sb)

-- lambda
unify (PLambdaType paramA returnA) (PLambdaType paramB returnB) =
  sParam <- unify paramB paramA
  sReturn <- unify (apply sParam ra) (apply sParam rb)
  return (sParam <+> s2)

-- if nothing matched it's an error
unify ta tb err = throwTE (printf err (show ta) (show tb))

-- perform occurs check
-- creates a substituion
varBind :: Name -> Type -> TC Substitution
varBind ide t | t == PTypeVar ide = return nullSubst
              | ide `Set.member` ftv t = throwTE $ "Occurs in type: " ++ ide ++ " vs. " ++ show t
              | otherwise = return $ Map.singleton ide t

\end{haskell}


\paragraph{Limitations of Hindley-Milner}
While the Hindley-Milner algorithm certainly is powerful and elegant, it has (in it's original form) some limitations that contrain the type system. Most notably we cannot support subtyping, meaning that we cannot define a type as being an extension of another. Subtyping in \code{Java} is declared using the \code{extends} keyword, thus making a \code{class} a subtype of another class (called the supertype).

Subtyping is not possible (or difficult at the least) due to the fact that the unification algorithm cannot tell the most general type of an expression if subtypes are allowed.

Consider the function $foo : Person \to Int$. Now, if we have the type $Manager <: Person$ (meaning $Manager$ is a subtype of $Person$) and we want to infer %TODO

%TODO: lexer ~ regex ~ lexeme
%TODO: lexical scoping
%TODO: optimization inlining


\subsection{Optimizer}

The optimizer implemented in Kite is currently only a simple dead-code elimination algorithm. The algorithm is given a starting declaration (usually the \code{main} declaration) and recursively traverses the AST beginning with the expressions defined in the starting node. When an identifier node is detected (except in a bind node) it's name is saved and recursion continues. The part of the AST that has been traversed, is the derivation tree that will be executed when running the program, thus all referenced identifiers will have been detected. The full list of declarations is now filtered by only persisting the ones that were detected during traversal, since we can be certain that they will never be accessed.

In it's current state the algorithm only eliminates unused top-level declarations, thus leaving unused locally scoped variables in the code. The algorithm can however be extended to eliminate local variables by transforming the AST during traversal. Using a stack of used identifiers we could, when entering a new local scope (a lambda or match case), push a new frame to the stack, add accessed variables found in the current scope to it, and when leaving the scope filter out the variables that were not accessed.

\subsection{Code generation}

The

TODO: (i language-deisgn) Write reserved names (Void, if, then, else, etc...)



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
